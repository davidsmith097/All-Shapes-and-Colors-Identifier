{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383c140c",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa430e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import cv2\n",
    "import pandas as pd\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46631f",
   "metadata": {},
   "source": [
    "# Prep Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570af100",
   "metadata": {},
   "source": [
    "I made a custom dataset because the data wasn't given in a way that lended itself to just a normal PyTorch dataset. I wanted to make it as easy as possible to turn the string labels into integers so my model could work with them, and I needed a way to store all of the images in a custom manner. The custom collate function was necessary because I split shape and color. I made this decision when I decided to have two fine-tuning heads in my model (which I'll explain the process of deciding later). The custom test set was necessary because there are no labels for the test set, so I needed a way to efficiently run the test images but not deal with labels that would've existed had I used my original CustomDataset.\n",
    "\n",
    "If I'd had test data that was labeled and I was actually trying to optimize based on the test set accuracy, I may have performed data augmentations. However, I didn't feel this was necessary for two main reasons. First, the dataset is relatively small, and accidentally over-augmenting the dataset could've really negatively impacted the model's performance, so it wasn't a risk I wanted to take. And second, I viewed some of the images and it seemed like the shapes were already fairly spread out and in a variety of orientations.\n",
    "\n",
    "I originally started with only labels for each shape and each color, but soon realized that it would make the model's architecture much simpler if I included a \"background\" class. I called this class \"NONE\", and filled any extra labels with that class assignment, so my model would learn to predict nothing, if no shape existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a793b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square = 0, circle = 1, triangle = 2\n",
    "# red = 0, green = 1, blue = 2\n",
    "LABELS = ['square', 'circle', 'triangle', 'NONE']\n",
    "COLORS = ['red', 'green', 'blue', 'NONE']\n",
    "\n",
    "def customCollate(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "\n",
    "    shapes = torch.stack([item[1][\"shape\"] for item in batch])\n",
    "    colors = torch.stack([item[1][\"color\"] for item in batch])\n",
    "\n",
    "    label_dict = {\"shape\": shapes, \"color\": colors}\n",
    "    return images, label_dict\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_dir, label_csv, transform=transforms.ToTensor(), max_shapes=10):\n",
    "        self.max_shapes = max_shapes\n",
    "        self.transform = transform\n",
    "        self.label_to_num = {\"square\" : 0, \"circle\" : 1, \"triangle\" : 2}\n",
    "        self.color_to_num = {\"red\" : 0, \"green\" : 1, \"blue\" : 2}\n",
    "        self.img_filenames = []\n",
    "        self.img_labels = []\n",
    "        \n",
    "        files = os.listdir(input_dir)\n",
    "        files = sorted(files, key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "        for img in files:\n",
    "            self.img_filenames.append(os.path.join(input_dir, img))\n",
    "        with open(label_csv, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            first = True\n",
    "            for row in reader:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "                self.img_labels.append(ast.literal_eval(row[1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.img_filenames[idx]\n",
    "        get_labels = self.img_labels[idx]\n",
    "\n",
    "        image = cv2.imread(img_filename)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        shape_list = []\n",
    "        color_list = []\n",
    "        for i in range(self.max_shapes):\n",
    "            if i < len(get_labels):\n",
    "                shape, col = get_labels[i]\n",
    "                shape_list.append(self.label_to_num[shape])\n",
    "                color_list.append(self.color_to_num[col])\n",
    "            else:\n",
    "                shape_list.append(3)\n",
    "                color_list.append(3)\n",
    "        label_dict = {\"shape\": torch.from_numpy(np.array(shape_list)), \"color\": torch.from_numpy(np.array(color_list))}\n",
    "\n",
    "        return image, label_dict\n",
    "\n",
    "class CustomTestSet(Dataset):\n",
    "    def __init__(self, input_dir, transform=transforms.ToTensor(), max_shapes=10):\n",
    "        self.image_dir = input_dir\n",
    "        self.max_shapes = max_shapes\n",
    "        self.transform = transform\n",
    "        self.img_filenames = []\n",
    "        \n",
    "        files = os.listdir(input_dir)\n",
    "        files = sorted(files, key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "        for img in files:\n",
    "            self.img_filenames.append(os.path.join(input_dir, img))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.img_filenames[idx]\n",
    "\n",
    "        image = cv2.imread(img_filename)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        relative_path = os.path.relpath(img_filename, start=self.image_dir)\n",
    "        relative_path = relative_path.replace(\"\\\\\", \"/\")\n",
    "        relative_path = f\"test_dataset/{relative_path}\"\n",
    "\n",
    "        return image, relative_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d4d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(r\"dataset_v3\\train_dataset\", r\"dataset_v3\\train.csv\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=customCollate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093d4cb",
   "metadata": {},
   "source": [
    "# Custom Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203946d0",
   "metadata": {},
   "source": [
    "I knew from the beginning that I wanted to use Cross-Entropy Loss, because I was doing multiclass classification. However, I eventually decided to perform the loss calculations separately. This decision was made when I changed my model to have two separate classification heads for shape and color. Originally, I had the total loss as a pure sum of the two losses, but I thought since color and shape were so closely related to each other, I would just average the two. If I'd found that my results were significantly skewed towards shape, I would have considered weighting each loss and then performing the sum. However, I deemed it necessary, since my by-hand inspection of the results seemed to yield results that weren't skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f324c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(pred, labels):\n",
    "    pred_shapes = pred[\"shape\"]\n",
    "    pred_colors = pred[\"color\"]\n",
    "    label_shapes = labels[\"shape\"]\n",
    "    label_colors = labels[\"color\"]\n",
    "\n",
    "    criterion_shape = nn.CrossEntropyLoss()\n",
    "    criterion_color = nn.CrossEntropyLoss()\n",
    "\n",
    "    shape_loss = criterion_shape(pred_shapes.reshape(-1, 4), label_shapes.reshape(-1))\n",
    "    color_loss = criterion_color(pred_colors.reshape(-1, 4), label_colors.reshape(-1))\n",
    "\n",
    "    return (shape_loss + color_loss) / 2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cbc1a",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c9ba5",
   "metadata": {},
   "source": [
    "I originally started with a model that had a ResNet50 backbone and only one classifier head on top. I attempted to train this a few times but my loss wasn't decreasing consistently. I found that the root cause was the lack of nonlinearity in my classifier, because it couldn't learn nonlinear features. So, I decided to add a second head (one for shape and one for color), and include nonlinearity. I found that the two heads, while not technically necessary (I could've just expanded the dimensions of one head to handle color and shape), it made performing the forward pass and calculating loss a lot easier.\n",
    "\n",
    "As specified in the Kaggle instructions, this model (and the CustomDataset) can be trained and tested with a customizable number of shapes that can be predicted. I looked briefly through the train and test data, and decided on 10 as a reasonable maximum. In order to predict more/less, the model would need to be retrained with that number passed as an argument.\n",
    "\n",
    "After deciding on a two-headed approach, and adapting my CustomDataset and customLoss function to handle that, I saw a better trend in my loss over ~10 epochs. But, it was still starting at a very high value, and not decreasing consistently (it would plateau around 0.3, after starting at 0.4). I realized that my CustomDataset was actually reading in the images in the wrong order. The line os.listdir() was reading them in string order, not in numerical order, like the labels were. So my bad trends in loss and overall wrong output was because I was training the model on incorrect labels. I fixed that, and while I was at it, added Kaiming initialization to my weights. In general, I've learned in my ML classes that randomly initializing weights leads to better performance, and I had used Kaiming initialization before, so I went with that.\n",
    "\n",
    "I did consider adding Dropout or a Batchnorm layer, but after Kaiming initialization and correcting the way I was handling the input images and labels, I had a much better trend in my loss, so I didn't think it was necessary.\n",
    "\n",
    "Originally, when my loss was plateauing around 0.3, I decided to add a learning rate scheduler to my optimizer, in case I was getting stuck at a local minimum. I like to use Cosine Annealing because I've encountered issues in the past where a scheduler that steps every epoch will reduce the learning rate too signficantly. I was plateauing early, so I wanted the steep drop at the beginning, and then for the learning rate to change only slightly once my loss started to get low. I think this was an advantageous addition to my training loop, but overall I know that fixing my image/labels issue is what made the most significant difference in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675fab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaimingInitWeights(seq_layer):\n",
    "    if isinstance(seq_layer, nn.Linear):\n",
    "        nn.init.kaiming_normal_(seq_layer.weight, nonlinearity='relu')\n",
    "        if seq_layer.bias is not None:\n",
    "            nn.init.constant_(seq_layer.bias, 0)\n",
    "\n",
    "\n",
    "class ShapeColorModel(nn.Module):\n",
    "    def __init__(self, max_shapes=10):\n",
    "        super().__init__()\n",
    "        self.max_shapes = max_shapes\n",
    "\n",
    "        # Get resnet50 but remove the last layer \n",
    "        self.backbone = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "        # To get output size (batch, 2048)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.backbone_dim = 2048\n",
    "\n",
    "        # One head to predict each of shape and color\n",
    "        self.shape_head = nn.Sequential(\n",
    "            nn.Linear(self.backbone_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.max_shapes * 4)\n",
    "        )\n",
    "\n",
    "        self.color_head = nn.Sequential(\n",
    "            nn.Linear(self.backbone_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.max_shapes * 4)\n",
    "        )\n",
    "        \n",
    "        self.shape_head.apply(kaimingInitWeights)\n",
    "        self.color_head.apply(kaimingInitWeights)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "    \n",
    "        # Get shape and color preds for each slot\n",
    "        out_shape = self.shape_head(features).view(-1, self.max_shapes, 4) # Get to shape (batch_size, max_shapes, 4)\n",
    "        out_color = self.color_head(features).view(-1, self.max_shapes, 4) # Get to shape (batch_size, max_shapes, 4)\n",
    "\n",
    "        return {\"shape\" : out_shape, \"color\" : out_color}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa89ac",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2aeab",
   "metadata": {},
   "source": [
    "I decided to train for only 50 epochs for two main reasons. First, the dataset is small, so training for too long would likely lead to overfitting, and ruin the ability of the model to generalize to data it hasn't seen before. And second, I noticed my loss was getting very low after ~30-40 epochs. I was initially worried that it had already overfit to the training data, but upon inspection of the outputs, I found that it was actually able to generalize fairly well, so I decided to keep training at 50 epochs.\n",
    "\n",
    "I also had access to a powerful GPU, so I made sure to run everything on CUDA, which saved me a lot of time. Moving every image and label from the CPU to GPU took a significant amount of time, but the overall difference it made when performing the backward pass was well worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815dc831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Avg Epoch Loss: 0.17995864152908325\n",
      "Avg Epoch Loss: 0.08428937196731567\n",
      "Avg Epoch Loss: 0.06089802086353302\n",
      "Avg Epoch Loss: 0.06278069317340851\n",
      "Avg Epoch Loss: 0.05843469500541687\n",
      "Avg Epoch Loss: 0.05334040895104408\n",
      "Avg Epoch Loss: 0.045295123010873795\n",
      "Avg Epoch Loss: 0.040356073528528214\n",
      "Avg Epoch Loss: 0.0399656742811203\n",
      "Avg Epoch Loss: 0.04457782581448555\n",
      "Avg Epoch Loss: 0.04277442768216133\n",
      "Avg Epoch Loss: 0.03840585798025131\n",
      "Avg Epoch Loss: 0.03518952801823616\n",
      "Avg Epoch Loss: 0.042878519743680954\n",
      "Avg Epoch Loss: 0.04186081141233444\n",
      "Avg Epoch Loss: 0.025221776217222214\n",
      "Avg Epoch Loss: 0.016185007989406586\n",
      "Avg Epoch Loss: 0.010437561199069023\n",
      "Avg Epoch Loss: 0.009701056405901909\n",
      "Avg Epoch Loss: 0.007365822326391935\n",
      "Avg Epoch Loss: 0.004836840555071831\n",
      "Avg Epoch Loss: 0.010315708816051483\n",
      "Avg Epoch Loss: 0.007665099110454321\n",
      "Avg Epoch Loss: 0.007265796419233084\n",
      "Avg Epoch Loss: 0.0039217788726091385\n",
      "Avg Epoch Loss: 0.0007361776079051197\n",
      "Avg Epoch Loss: 0.0018080672016367316\n",
      "Avg Epoch Loss: 0.0011371745495125651\n",
      "Avg Epoch Loss: 0.00048106900067068636\n",
      "Avg Epoch Loss: 0.00024154172569978982\n",
      "Avg Epoch Loss: 0.00016053058789111674\n",
      "Avg Epoch Loss: 8.700813486939296e-05\n",
      "Avg Epoch Loss: 5.76254133193288e-05\n",
      "Avg Epoch Loss: 5.789839633507654e-05\n",
      "Avg Epoch Loss: 5.6636668887222186e-05\n",
      "Avg Epoch Loss: 3.261125675635412e-05\n",
      "Avg Epoch Loss: 0.0001597034279257059\n",
      "Avg Epoch Loss: 6.008722630213015e-05\n",
      "Avg Epoch Loss: 0.0003830036148428917\n",
      "Avg Epoch Loss: 0.00028819471481256187\n",
      "Avg Epoch Loss: 4.4033004087395966e-05\n",
      "Avg Epoch Loss: 4.9519119784235954e-05\n",
      "Avg Epoch Loss: 3.6648500099545345e-05\n",
      "Avg Epoch Loss: 2.965290696010925e-05\n",
      "Avg Epoch Loss: 2.7798629162134603e-05\n",
      "Avg Epoch Loss: 2.53578728006687e-05\n",
      "Avg Epoch Loss: 2.6573252398520708e-05\n",
      "Avg Epoch Loss: 4.194913708488457e-05\n",
      "Avg Epoch Loss: 2.3251235688803717e-05\n",
      "Avg Epoch Loss: 3.645770266302861e-05\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "model = ShapeColorModel()\n",
    "model.cuda()\n",
    "\n",
    "print(next(model.shape_head.parameters()).device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=0.00001)\n",
    "\n",
    "model.train()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = []\n",
    "    # print(\"Entering batches\")\n",
    "    for i, (batch, labels) in enumerate(train_dataloader):\n",
    "        # print(\"Starting batch to cuda\")\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        # print(\"Done with batch loading\")\n",
    "        labels = {k: v.to(device, non_blocking=True) for k, v in labels.items()}\n",
    "\n",
    "        # print(\"Forward pass\")\n",
    "        pred = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = customLoss(pred, labels)\n",
    "        # print(\"Post loss\")\n",
    "        loss.backward()\n",
    "        # print(\"Post backward pass\")\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss.append(loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"Avg Epoch Loss: {sum(epoch_loss) / len(epoch_loss)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233035e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0205d",
   "metadata": {},
   "source": [
    "For inference, I argmaxed on the dimension with the class assignments. If the model predicted background, I ignored the prediction. If the model predicted the same shape twice, I also ignored the prediction (as was specified in the Kaggle instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomTestSet(r\"dataset_v3\\test_dataset\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, (batch, label) in enumerate(test_dataloader):\n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "\n",
    "        pred = model(batch)\n",
    "        shape_preds = pred[\"shape\"].squeeze(0).argmax(dim=1)\n",
    "        color_preds = pred[\"color\"].squeeze(0).argmax(dim=1)\n",
    "\n",
    "        shape_preds = shape_preds.cpu().numpy()\n",
    "        color_preds = color_preds.cpu().numpy()\n",
    "\n",
    "        label_list = []\n",
    "        for shape, color in zip(shape_preds, color_preds):\n",
    "                if shape == 3 or color == 3:\n",
    "                        continue\n",
    "                if (LABELS[shape], COLORS[color]) in label_list:\n",
    "                      continue\n",
    "                label_list.append((LABELS[shape], COLORS[color]))\n",
    "        predictions.append((label, str(label_list)))\n",
    "\n",
    "with open(\"test_predictions_final.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow([\"img_path\", \"label\"])\n",
    "    for filename, label_string in predictions:\n",
    "        writer.writerow([filename[0], label_string])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56834a",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d4cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().state_dict(), 'model_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
